{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "\n",
    "import os\n",
    "from os.path import join\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from networks.spectral import spectral_ae\n",
    "\n",
    "from training.model_config import AttrDict\n",
    "from training.spectral import data_loader\n",
    "from training.spectral.log import *\n",
    "from training.spectral.loss import loss_function\n",
    "\n",
    "from utils.utils import export_obj\n",
    "\n",
    "# Suppress Tensorflow 2.0.0 deprecation warnings.\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the mesh data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# Define the training parameters.\n",
    "def define_config():\n",
    "    config = AttrDict()\n",
    "    \n",
    "    config.latent_variable = 64\n",
    "    config.filters = [16, 32, 32, 48]\n",
    "    config.sampling_steps = len(config.filters)\n",
    "    config.poly_order = [3] * config.sampling_steps\n",
    "\n",
    "    config.n_epochs = 1\n",
    "    config.batch_size = 64\n",
    "    config.lr = 0.001\n",
    "    config.l2_reg = 0.00005\n",
    "    config.z_l2_penalty = 0.0000005\n",
    "\n",
    "    config.batch_norm = False\n",
    "    config.residual = False\n",
    "\n",
    "    config.type = 'sampling_{}'.format(config.sampling_steps)\n",
    "    config.optimizer = 'AdamW'\n",
    "\n",
    "    config.info = \"data: synth\"\n",
    "    return config\n",
    "\n",
    "config = define_config()\n",
    "latent_variable, sampling_steps, filters, n_epochs = config.latent_variable, config.sampling_steps, config.filters, config.n_epochs\n",
    "poly_order, batch_size, lr, l2_reg, batch_norm = config.poly_order, config.batch_size, config.lr, config.l2_reg, config.batch_norm\n",
    "z_l2_penalty = config.z_l2_penalty\n",
    "\n",
    "model_id = 'z64_d4_1550943246.5350132'\n",
    "\n",
    "# Define file paths.\n",
    "ROOT = '..'\n",
    "DATA_DIR = 'data'\n",
    "DATASET_PATH = join(ROOT, DATA_DIR, 'datasets/mesh-samples', 'data_splits_sampler.pkl')\n",
    "\n",
    "TENPLATE_DATA_PATH = join(ROOT, DATA_DIR, 'template')\n",
    "GRAPH_STRUCTURE = join(TENPLATE_DATA_PATH, config.type)\n",
    "TRILIST_PATH = join(TENPLATE_DATA_PATH, 'trilist.npy')\n",
    "OUTPUT_PATH = join(ROOT, DATA_DIR, 'models/spectral-ae', model_id)\n",
    "CHECKPOINT_PATH = join(OUTPUT_PATH, 'models')\n",
    "\n",
    "# Get the trilist for exporting the meshes.\n",
    "trilist = np.load(TRILIST_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_db_np, mean_points, std_points = data_loader.load_test_data(DATASET_PATH)\n",
    "\n",
    "# Create dataset iterators\n",
    "test_db, fp_test, lp_test = data_loader.create_dataset(test_db_np, n_epochs=1, batch_size=batch_size, reshuffle=False)\n",
    "\n",
    "handle, iterator = data_loader.create_feedable_iterator(test_db)\n",
    "next_X, next_Y = iterator.get_next()\n",
    "test_db_it = test_db.make_initializable_iterator()\n",
    "\n",
    "# Load spectral operators.\n",
    "L, A, D, U, p = data_loader.load_spectral_operators(GRAPH_STRUCTURE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build the spectral autoencoder.\n",
    "is_train = tf.placeholder(tf.bool, name=\"is_train\")\n",
    "net, mesh_embedding = spectral_ae.build_model(next_X, L, D, U, A, filters, latent_variable, poly_order, lr, is_train, batch_norm=batch_norm)\n",
    "loss = loss_function(net, next_Y, l2_reg, z_l2_penalty, mesh_embedding)\n",
    "\n",
    "global_step = tf.Variable(0, name='global_step', trainable=False)\n",
    "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.control_dependencies(update_ops):\n",
    "    opt = tf.contrib.opt.AdamWOptimizer(learning_rate=lr, weight_decay=0.000001).minimize(loss, global_step=global_step)\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    test_handle = sess.run(test_db_it.string_handle())\n",
    "\n",
    "    sess.run(init, feed_dict={handle: test_handle})\n",
    "    sess.run(test_db_it.initializer, feed_dict={fp_test: test_db_np, \n",
    "                                                lp_test: test_db_np})\n",
    "\n",
    "    saver = tf.train.Saver(max_to_keep=5, keep_checkpoint_every_n_hours=2)\n",
    "    saver.restore(sess, join(CHECKPOINT_PATH, 'mesh_ae.1550965497.0982995-599'))\n",
    "\n",
    "    predictions, gt_X, label = sess.run([net, next_X, next_Y], feed_dict={handle: test_handle, \n",
    "                                                                          is_train: False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstruct meshes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PREDICTIONS_PATH = join(OUTPUT_PATH, 'mesh-predictions')\n",
    "\n",
    "reconstruction = predictions * std_points + mean_points\n",
    "gt = gt_X * std_points + mean_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_samples = 8\n",
    "for index in range(n_samples):\n",
    "    \n",
    "    export_obj(reconstruction[index], trilist, \n",
    "                     join(PREDICTIONS_PATH, '{}_pred.obj'.format(index)))\n",
    "\n",
    "    export_obj(gt[index], trilist, \n",
    "                     join(PREDICTIONS_PATH, '{}_gt.obj'.format(index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encode meshes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, join(CHECKPOINT_PATH, 'mesh_ae.1550965497.0982995-599'))\n",
    "    \n",
    "    embedding = mesh_embedding.eval(feed_dict={next_X: gt_X[:32], is_train: False})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interpolate meshes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INTERPOLATE_PATH = join(OUTPUT_PATH, 'mesh-interpolate')\n",
    "\n",
    "n_iterations = 5\n",
    "source = embedding[5]\n",
    "target = embedding[10]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, join(CHECKPOINT_PATH, 'mesh_ae.1550965497.0982995-599'))\n",
    "\n",
    "    for iteration in range(n_iterations + 1):\n",
    "        z_interpolate = source + (target - source) * iteration / n_iterations\n",
    "        z_interpolate = np.expand_dims(z_interpolate, axis=0)\n",
    "        output_interpolate = net.eval(feed_dict={mesh_embedding: z_interpolate, is_train: False})\n",
    "        \n",
    "        mesh_interpolate = output_interpolate[0] * std_points + mean_points\n",
    "        \n",
    "        export_obj(mesh_interpolate, trilist, \n",
    "                 join(INTERPOLATE_PATH, '{}.obj'.format(iteration)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize the latent space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mayavi import mlab\n",
    "\n",
    "from traits.api import HasTraits, Range, Instance, on_trait_change, Button, Enum, Bool\n",
    "from traitsui.api import View, Item, Group, VGroup, HGroup, HSplit, Tabbed\n",
    "from mayavi.core.ui.api import MayaviScene, SceneEditor, MlabSceneModel\n",
    "from traitsui.menu import RevertButton\n",
    "from traits.api import HasTraits, Property, Array, Font\n",
    "from traitsui.api import View, Item, TabularEditor\n",
    "from traitsui.tabular_adapter import TabularAdapter\n",
    "\n",
    "class Visualization(HasTraits):\n",
    "    weird_hack =  \",\".join([\"shape_{}\".format(ii) for ii in range(latent_variable)])\n",
    "    \n",
    "    scene = Instance(MlabSceneModel, ())\n",
    "    btn_reset_shape = Button('Reset Shape')\n",
    "    pose_deformations = Bool\n",
    "    \n",
    "    shape_sliders = [Item('shape_' + str(ii)) for ii in range(latent_variable)]\n",
    "    shape_group = VGroup(shape_sliders,\n",
    "                         Item('btn_reset_shape', show_label=False),\n",
    "                         label='Shape')\n",
    "    \n",
    "    view = View(HSplit(\n",
    "        shape_group,\n",
    "        Item('scene', editor=SceneEditor(scene_class=MayaviScene), height=550, width=750, show_label=False)),\n",
    "                resizable=True)\n",
    "\n",
    "    def __init__(self, sess, embedding):\n",
    "        HasTraits.__init__(self)\n",
    "        self.betas = embedding\n",
    "        self.sess = sess\n",
    "        RANGE_INT = 4.\n",
    "        for ii in range(latent_variable):\n",
    "            R = Range(-RANGE_INT, RANGE_INT, embedding[0, ii])\n",
    "            self.add_trait(\"shape_\" + str(ii), R)\n",
    "\n",
    "        init_v = net.eval(feed_dict={mesh_embedding: embedding, is_train: False})\n",
    "        self.v = init_v[0] * std_points + mean_points\n",
    "        \n",
    "        x, y, z, f = self.v[:, 0], self.v[:, 1], self.v[:, 2], trilist\n",
    "        self.mesh = mlab.triangular_mesh(x, y, z, f, figure=self.scene.mayavi_scene, color=(0, 1, 1)) \n",
    "    \n",
    "    def update_betas(self):\n",
    "        self.betas[0, :] = [getattr(self, \"shape_{}\".format(ii)) for ii in range(latent_variable)]    \n",
    "    \n",
    "    def update_plot(self):\n",
    "        output = net.eval(feed_dict={mesh_embedding: self.betas, is_train: False})\n",
    "        self.v = output[0] * std_points + mean_points\n",
    "        x, y, z = self.v[:, 0], self.v[:, 1], self.v[:, 2]\n",
    "        self.mesh.mlab_source.set(x=x, y=y, z=z)\n",
    "\n",
    "    @on_trait_change(weird_hack)\n",
    "    def shape_sliders_action(self):\n",
    "        self.update_betas()\n",
    "        self.update_plot()   \n",
    "        \n",
    "    def _btn_reset_shape_fired(self):\n",
    "        self.betas = np.zeros((1, latent_variable))\n",
    "        for ii in range(latent_variable):\n",
    "            setattr(self, \"shape_{}\".format(ii), 0) \n",
    "        self.update_betas()\n",
    "        self.update_plot()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The UI was implemented for a smaller latent vector so the scroll view might be required.\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    saver.restore(sess, join(CHECKPOINT_PATH, 'mesh_ae.1550965497.0982995-599'))\n",
    "    z = embedding[0].reshape(1, -1).copy()\n",
    "#     z = np.zeros((1, latent_variable))\n",
    "    Visualization(sess, z).configure_traits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
